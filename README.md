# Sinhala Toxic Comment Classification using XLM-RoBERTa

This project demonstrates the fine-tuning of the multilingual transformer model `xlm-roberta-large` for classifying Sinhala social media comments as toxic or non-toxic.

---

## ğŸš€ Project Overview

- **Objective**: Build a robust classifier to detect toxic comments in Sinhala, a low-resource language.
- **Model**: `xlm-roberta-large`
- **Dataset**: Custom-labeled Sinhala comments scraped from social platforms
- **Output**: Binary classification (Toxic = 1, Non-toxic = 0)

---

## ğŸ§  Key Features

- Tokenization analysis to choose optimal sequence length
- Basic data augmentation using `[MASK]` token injection
- Early stopping for training efficiency
- Mixed-precision (`fp16`) training for GPU speedup
- Custom dataset class with HuggingFace `Trainer` API
- Evaluation with F1-score, confusion matrix, and classification report

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ cleaned_sinhala_comments.csv       # Preprocessed dataset
â”œâ”€â”€ fine_tuned_sinhala_model/         # Saved tokenizer and model
â”œâ”€â”€ sinhala_toxic_detection.ipynb     # Full notebook with code
â”œâ”€â”€ logs/                             # Training logs
â””â”€â”€ model/                            # Output directory during training
```

---

## âš™ï¸ Setup Instructions

```bash
# Clone the repository
$ git clone https://github.com/yourusername/sinhala-toxic-comment-classifier.git
$ cd sinhala-toxic-comment-classifier

# Install required packages
$ pip install -r requirements.txt
```

---

## ğŸ§ª Training Configuration

```python
TrainingArguments(
    output_dir='./model',
    num_train_epochs=3,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    fp16=True,
    eval_strategy="steps",
    save_strategy="steps",
    evaluation_strategy="steps",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    gradient_accumulation_steps=2,
    learning_rate=2e-5,
    lr_scheduler_type="cosine",
    seed=42
)
```

---

## ğŸ“Š Evaluation Metrics

- **Accuracy**, **F1-score**, **Precision**, **Recall**
- Confusion Matrix using `matplotlib` + `seaborn`
- Classification Report with `sklearn`

---

## ğŸ” Example Predictions

```python
Text: à¶‹à¶¹ à·€à¶œà·š à¶œà·œà¶±à·Šà¶¢à¶ºà·™à¶šà·Šà¶§ à¶šà·œà·„à·™à¶¯ à¶¸à·™à¶ à·Šà¶ à¶» à·„à·’à¶­à·™à¶± à·„à·à¶§à·’
Prediction: Toxic (1) | Confidence: 0.94

Text: 33à¶šà·Š à·€à¶œà·šà¶¸ à·„à·œà¶³à¶ºà·’, peaceful vibe à¶‘à¶šà¶šà·Š à¶­à·’à¶ºà·™à¶±à·€à·
Prediction: Non-Toxic (0) | Confidence: 0.89
```

---

## ğŸŒ Multilingual & Inclusivity Impact

This project tackles the challenge of Sinhala NLP using a multilingual transformer model, addressing:

- Language inclusivity
- Toxic speech moderation in underrepresented communities
- Transformer-based adaptation for low-resource languages

---

## ğŸ› ï¸ Optimizations

- Mixed-precision (`fp16`) training for faster convergence
- Token length truncation based on 95th percentile analysis
- Model evaluation pipeline ready for edge deployment

---

## ğŸ“ˆ Future Improvements

- Integrate with WandB for experiment tracking
- Quantize model for mobile/edge deployment
- Use LoRA or DistilRoBERTa for smaller variants
- Incorporate advanced augmentation (e.g., back-translation)

---

## ğŸ“Œ Related Skills

- HuggingFace Transformers
- PyTorch
- NLP for low-resource languages
- Model interpretability
- Text classification pipeline

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to fork the repo and submit pull requests.

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ“¬ Contact

If you have any questions or want to collaborate:

- GitHub: [yourusername](https://github.com/yourusername)
- Email: [yourname@example.com](mailto\:yourname@example.com)

---



